> # 第三讲 基于 InternLM 和 LangChain 搭建你的知识库
> <p>主讲人：邹雨衡</p> <p>笔记记录人：ZK-Jackie</p> <p>笔记记录时间：2024.2.7</p>

## 目录
- **一、前置知识**
  - **1. LLM的局限性**
  - **2. 突破LLM局限的方法**
- **二、LangChain和基于LangChain搭建本地应用介绍**
  - **1. 基本介绍**
  - **2. 基于LangChain搭建本地应用的基本流程**
- **三、总结**
- **四、课后作业**

## 一、前置知识
### 1. LLM的局限性
- 知识时效性受限：如何让LLM能够获取最新的知识
- 专业能力有限：如何打造垂域大模型
- 定制化成本高：如何打造个人专属的LLM应用
### 2. 突破LLM局限的方法
为了解决LLM存在的上述问题，当下也有两种基本思路解决它，分别是通过Finetune（微调）和RAG（检索增强生成）。

#### Finetune（微调）
核心思路：在较小的训练集中，通过微调的方式，让模型适应特定的领域。

优势：
- 可个性化微调
- 知识覆盖面广
- 成本高昂
- 无法实时更新

#### RAG（检索增强生成）
<p>核心思路：外挂知识库，基于检索的生成式模型，能够在生成过程中引入检索的信息，从而提高生成的质量，其原理可如下图所示：</p>
<img src="../assets/internlm_study/03/1.png" alt="图 1 RAG范式原理" style="width: 60%">

优势：
- 低成本
- 可实时更新

劣势：
- 受基座模型影响大
- 单次回答知识有限

<p>本次课程中也将基于RAG范式，学习如何通过LangChain搭建知识库。</p>

## 二、LangChain和基于LangChain搭建本地应用介绍
### 1. 基本介绍
LangChain 框架是一个开源工具，通过为各种 LLM 提供通用接口来简化应用程序的开发流程，帮助开发者自由构建 LLM应用。LangChain 的核心组成模块即为:
- 链(Chains):将组件组合实现端到端应用，通过一个对象封装实现一系列LLM操作（如检索问答链，覆盖实现了 RAG(检索增强生成)的全部流程）
### 2. 基于LangChain搭建本地应用的基本流程

<img src="../assets/internlm_study/03/2.png" alt="图 2 基于LangChain搭建本地应用流程" style="width: 60%">

步骤：

#### （1）基于个人数据构建向量数据库
这一环节大致有三个步骤：
- 加载源文件
- 文档分块
- 文档向量化

##### 加载源文件（Unstructured Loader）
在确定源文件类型后，需要针对不同类型源文件选用不同的加载器。这一过程的核心在于将带格式文本转化为无格式字符串。

##### 文档分块（Text Spliter）
大模型输入的上下文往往是有限的，而由于单个文档往往超过模型的上下文上限，则我们需要对加载的文档进行切分。这一过程一般按字符串长度进行分割，也可以可以手动控制分割块的长度和重鲁区间长度。分割完成后，会获得多个Text Chunk。

##### 文档向量化（Sentence Transformer）
对文本进行一定处理后，我们需要确定 chunk 之间的关系，这一过程即文档向量化，构建向量数据库，使用向量数据库来支持语义检索。要实现这一过程，可以使用任一一种 Embedding 模型来进行向量化，如 Sentence Transformer；也可以使用多种支持语义检索的向量数据库，如轻量级的 Chroma

#### （2）基于InternLM模型搭建知识库助手
首先，我们需要将InternLM接入LangChain。LangChain支持自定义LLM，可以直接接入到框架中我们只需将InternLM部署在本地，并封装一个自定义LLM类，调用本地InternLM即可。

LangChain提供了检索问答链模版，可以自动实现知识检索、Prompt 嵌入、LLM问答的全部流程。随后将基于InternLM的自定义LLM和已构建的向量数据库接入到检索问答链的上游调用检索问答链，即可实现知识库助手的核心功能

<img src="../assets/internlm_study/03/3.png" alt="图 3 检索问答链" style="width: 60%">

#### （3）RAG优化
在构建了自己个人的知识库助手后，这样一个基于RAG的问答系统性能核心可能会受限于：

- 检索精度
- Prompt性能

而我们也能在检索和Prompt方面给出相应的优化点：

- 检索方面
  - 切分过程中，基于语义进行分割，保证每一个chunk的语义完整
  - 生成chunk后，给每一个chunk生成概括性索引，检索时匹配索引
- Prompt方面
  - 迭代优化Prompt策略

#### （4）Web Demo部署
在完成了本地应用的核心部分搭建后，我们可以将其部署到Web端，这样我们就可以通过浏览器访问我们的知识库助手了。当下，有众多支持简易Web部署的框架，如Gradio、Streamlit等，我们可以选择其中一种框架进行部署，下面将使用Gradio框架进行部署。

## 三、总结
本次课程深入探讨了大型语言模型（LLM）的局限性以及突破这些局限性的方法，特别是通过Finetune（微调）和RAG（检索增强生成）两种策略。此外，课程还介绍了如何利用LangChain框架来搭建基于LLM的本地应用，从而实现个性化、实时更新的知识库助手。

### 1. LLM的局限性
- 知识时效性受限：LLM难以获取最新的知识，这限制了其在快速变化的领域中的应用。
- 专业能力有限：LLM在特定垂直领域的专业能力不足，需要进一步的优化和定制。
- 定制化成本高：打造个人专属的LLM应用成本较高，技术门槛也相对较高。


### 2. 突破LLM局限的方法
- Finetune（微调）：通过在特定领域的小规模数据集上进行微调，提升LLM在该领域的表现。
- RAG（检索增强生成）：通过外挂知识库的方式，在生成过程中引入检索到的信息，提高回答的质量和时效性。


### 3. LangChain的应用
LangChain框架为开发者提供了一个简化的接口，通过链（Chains）的方式组合各种组件，实现端到端的LLM应用。本课程详细介绍了基于LangChain搭建本地应用的流程，包括构建向量数据库、搭建知识库助手、RAG优化以及Web Demo部署等步骤，为开发者提供了一条从零开始构建个性化LLM应用的路径。

### 4. 结论
通过本次课程的学习，我们不仅了解了LLM的局限性及其解决方案，还掌握了如何利用LangChain框架搭建基于LLM的应用。这为我们在未来的项目中提供了宝贵的指导和灵感，尤其是在构建个性化、实时更新的知识库助手方面。随着LLM技术的不断发展，我们期待看到更多创新的应用场景，以及它们如何为人类社会带来正面的影响。

## [四、课后作业](03_2.md)