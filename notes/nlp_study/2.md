<div style="border-bottom: 4px solid black; width: 100%; box-sizing: border-box; text-align: center; padding-top: 0.1rem;" align="center">
    <h1>Datawhale AI夏令营实战笔记<br/><span>Task2：从baseline代码详解入门深度学习</span></h1>
</div>
<div style="text-align: center;" align="center">
    笔记记录人：ZK-Jackie&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;笔记记录时间：2024.7.13
</div>

## 目录
- [一、baseline代码详解](#一baseline代码详解)
  - [1. 数据预处理](#1-数据预处理)
  - [2. 模型搭建](#2-模型搭建)
  - [3. 模型训练](#3-模型训练)
  - [4. 模型评估](#4-模型评估)
- [二、总结](#二总结)
- [参考文章](#参考文章)

## 一、baseline代码详解

baseline是指在比赛开始之前，组织方提供的一个基础的解决方案，参赛选手可以在此基础上进行修改和优化。在本次实战中，我们将从baseline代码详解入手，介绍使用深度学习的办法解决机器翻译问题。

### 1. 数据预处理

数据预处理是指将原始数据转换为模型可以接受的数据格式。观察本次赛方提供的训练数据，我们可以发现数据格式为：

```text
    ...
<English>\t<Chinese>
    ...
```

其中`\t`表示制表符，`<English>`表示英文句子，`<Chinese>`表示中文句子。赛方给出了诸多如此形式的句段，还给出了一个专业词汇数据集，希望加入了专业词汇数据集的神经网络将能够更加准确地实现翻译。正式开始构建我们的神经网络前，我们首先需要处理这些文本数据。

数据清洗主要是对原始数据进行处理，包括去除无效字符、分词、去除停用词等。详细观察数据集中的内容，我们可以发现数据集中存在大量的标点符号、数字、英文大小写、其他不必要的词等无效信息，这些信息对于神经网络的训练并无帮助，因此我们需要对这些信息进行清洗。

> 1. 去除冗余字符
> - 去除无关字符：去除标点符号、数字、英文大小写等无关字符。
> - 去除停用词：去除中文停用词，如“的”、“是”、“在”等。
> 2. 分词处理
> - 英文分词：使用nltk、spacy等对英文句子进行分词处理。
> - 中文分词：使用jieba、hanlp等对中文句子进行分词处理。
> 3. 构建词向量和词典
> - 构建词向量：将文本数据转换为词向量表示。
> - 构建词典：构建英文词典和中文词典，用于将文本数据转换为模型可以接受的数据格式，为每一个词分配一个唯一的索引。
> 4. 数据集划分
> - 划分训练集和验证集：将数据集划分为训练集和验证集，用于模型的训练和评估。
> - 划分批次数据：将数据集划分为多个批次数据，用于模型的训练。
> 5. 序列截断和填充
> - 序列截断：对文本数据进行截断处理，保留关键信息。
> - 序列填充：对文本数据进行填充处理，保持数据长度一致。
> 6. 特殊标记
> - 添加起始标记和终止标记：在句子的开头和结尾添加特殊标记，用于模型的训练。
> - 添加未知标记：对于未在词典中出现的词，添加未知标记，用于模型的训练。
> 7. 数据加载
> - 加载数据：使用torch.utils.data.Dataset加载数据集，用于模型的训练。
> - 数据迭代：使用torch.utils.data.DataLoader迭代数据集，用于模型的训练。
> - 数据可视化：使用matplotlib、seaborn等对数据进行可视化展示。
> 8. 数据增强
> - 数据增强：对数据进行增强处理，如随机翻译、随机替换、随机删除等。
> - 数据扩充：对数据进行扩充处理，如数据重复、数据旋转、数据缩放等。
> - 数据平衡：对数据进行平衡处理，使正负样本比例均衡。

### 2. 模型搭建

对训练数据进行预处理后，我们需要构建一个神经网络模型，用于实现机器翻译的功能。在本次实战中，我们将使用Seq2Seq模型实现机器翻译的功能。Seq2Seq模型是一种端到端的序列到序列模型，其核心思想是通过编码器将源语言句子编码为一个固定长度的向量表示，然后通过解码器将这个向量表示解码为目标语言句子。

在编码器中，我们可以使用RNN、LSTM、GRU等循环神经网络模型，用于将源语言句子编码为一个固定长度的向量表示。在解码器中，我们可以使用RNN、LSTM、GRU等循环神经网络模型，用于将这个向量表示解码为目标语言句子。

当编码器获得了一个字符串后，它会将其转换为一个向量，这个向量会被传递给解码器，解码器会将这个向量转换为目标语言的字符串。在这个过程中，编码器和解码器会通过训练来学习如何将一个字符串转换为另一个字符串。

但是，Seq2Seq模型存在一个问题，即无法处理长距离依赖关系。长距离依赖关系是指源语言句子和目标语言句子之间存在较长的距离，这种情况下，编码器和解码器之间的信息传递会受到限制，导致翻译质量下降。

为了解决这个问题，我们可以使用注意力机制，通过注意力机制来实现对源语言句子的不同部分进行加权，从而提高模型的翻译质量。注意力机制引入后，使得不再需要把原始文本中的所有必要信息压缩到一个向量当中，而是可以根据需要从原始文本中选择性地提取信息。

### 3. 模型训练

模型搭建完成后，我们需要对模型进行训练，以便使其能够准确地实现机器翻译的功能。在模型训练过程中，我们需要定义损失函数、优化器和评价指标，用于评估模型的性能。

损失函数是用来衡量模型预测值与真实值之间的差异，优化器是用来更新模型参数，评价指标是用来评估模型的性能。在模型训练过程中，我们需要根据损失函数计算损失值，然后使用优化器更新模型参数，最后根据评价指标评估模型的性能。

### 4. 模型评估

模型训练完成后，我们需要对模型进行评估，以便了解模型的性能。在模型评估过程中，我们需要使用验证集对模型进行评估，计算模型的损失值和评价指标，以便了解模型的性能。

本次比赛及训练过程中，主要采用BLEU指标对模型进行评估。BLEU指标是一种用于评估机器翻译质量的指标，其核心思想是通过比较模型生成的翻译结果与参考翻译结果之间的相似度，从而评估模型的翻译质量。

## 二、总结

本次实战中，我们从baseline代码详解入手，介绍了使用深度学习的办法解决机器翻译问题。通过数据预处理、模型搭建、模型训练和模型评估等步骤，我们可以构建一个准确、高效的机器翻译模型，实现中英文之间的翻译功能。

## 参考文章

- [从零入门NLP竞赛 - Datawhale AI 夏令营](https://datawhaler.feishu.cn/wiki/TObSwHZdFi2y0XktauWcolpcnyf)
- [Task2：从baseline代码详解入门深度学习](https://datawhaler.feishu.cn/wiki/PztLwkofsi95oak2Iercw9hkn2g)