<div style="border-bottom: 4px solid black; width: 100%; box-sizing: border-box; text-align: center; padding-top: 0.1rem;" align="center">
    <h1>Datawhale AI 夏令营 LLM 应用开发实战笔记<br/><span>Task1：了解模型获取和 IPEX-LLM 部署工具</span></h1>
</div>
<div style="text-align: center;" align="center">
    笔记记录人：ZK-Jackie&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;笔记记录时间：2024.7.14
</div>

## 目录
- [一、赛题分析](#一赛题分析)
  - [1. 赛题背景](#1-赛题背景)
  - [2. 赛题要求](#2-赛题要求)
  - [3. 评测指标](#3-评测指标)
- [二、模型获取](#二模型获取)
  - [1. 模型获取方式](#1-模型获取方式)
  - [2. 模型部署方式](#2-模型部署方式)
- [三、IPEX-LLM 部署工具](#三ipex-llm-部署工具)
  - [1. IPEX-LLM 简介](#1-ipex-llm-简介)
  - [2. 使用 IPEX-LLM 部署 LLM 步骤](#2-使用-ipex-llm-部署-llm-步骤)
- [四、总结](#四总结)
- [参考文章](#参考文章)

## 一、赛题分析

### 1. 赛题背景

LLM 是一种基于大规模预训练语言模型的语言生成模型，具有强大的生成能力和泛化能力。本次赛题旨在通过 LLM 模型生成文本，实现对话系统的功能。近年来，随着深度学习技术的发展，基于 LLM 的对话系统在自然语言处理领域取得了显著的进展，成为了对话系统研究的热点之一。

但是，由于 LLM 模型参数庞大、计算资源消耗大、部署复杂等，常常需占用较大的 GPU 资源，且 GPU 的使用成本较高。因此，如何高效地利用有限的计算资源，提高 LLM 模型的部署效率，成为了当前研究的热点问题之一。Intel 为了解决这一问题，推出了能使大模型在CPU上进行推理的 IPEX-LLM 部署工具，旨在提高 LLM 模型的部署效率，降低部署成本。

### 2. 赛题要求

本次赛题要求使用指定的开源大模型，采用基于 Intel 的部署及优化工具，并结合多种多样的工具，完成具有创新型、实用性的应用开发。具体要求如下：

> 1. 可用的大模型
>    1. 通义千问Qwen系列
>    2. 智谱ChatGLM及GLM4系列
>    3. 百川2系列
>    4. 其他经过 Intel 发布的各种模型的微调版本
> 2. 部署及优化工具（至少使用一个）
>    1. OpenVINO™ Toolkit
>    2. ++IPEX-LLM (Intel® LLM Library for PyTorch)
>    3. Intel® Extension for Transformers ++或其组件 (Neural Chat)++
>    4. xFasterTransformer
>    5. Intel Extension for Pytorch
> 3. 可选开发工具
>    1. 权重调整
>    2. 低精度量化
>    3. RAG
>    4. 其他各种模型微调及优化技术

### 3. 评测指标

本次赛题的评测指标主要包括模型的生成质量、生成速度、部署效率等。具体评测指标如下：

> 初赛评选标准：
> 创意和应用价值：
> - 创意性： 作品的创新性和独特性，包括对大语言模型的应用创意和解决方案的独特性。
> - 应用价值： 作品对特定领域的应用潜力和价值，如健康服务、适老服务、教育等，是否满足社会发展需要，以及是否能够在行业快速落地。
> 
> 技术实现和优化利用：
> - 技术特点和工具使用： 作品中使用的技术特点和工具，包括是否有效地利用了英特尔的开源软件优化工具（参见推荐使用的部署及优化工具）。

## 二、模型获取

### 1. 模型获取方式

本次赛题要求使用指定的开源大模型，可通过以下方式获取：

- 从 huggingface 获取模型
  - 通过 huggingface-cli 软件工具，终端输入命令下载模型
  - 通过 python huggingface SDK，使用 snapshot 下载
- 从 modelscope 获取模型
  - 通过 modelscope 软件工具，终端输入命令下载模型
  - 通过 python modelscope SDK，使用 snapshot 下载

### 2. 模型部署方式

本次赛题要求使用指定的部署及优化工具，可通过以下工具部署模型，详细使用说明可参考下面给定的链接：

<table>
    <tr>
        <th>部署工具</th>
        <th>详细介绍</th>
    </tr>
    <tr>
        <td>OpenVINO™ Toolkit</td>
        <td>https://github.com/openvinotoolkit/openvino</td>
    </tr>
    <tr>
        <td>++IPEX-LLM (Intel® LLM Library for PyTorch)</td>
        <td>https://github.com/intel-analytics/ipex-llm/</td>
    </tr>
    <tr>
        <td>Intel® Extension for Transformers ++或其组件 (Neural Chat)++</td>
        <td>https://github.com/intel/intel-extension-for-transformers
        <p>https://medium.com/intel-analytics-software/accelerating-qwen2-models-with-intel-extension-for-transformers-99403de82f68</p>
        </td>
    </tr>
    <tr>
        <td>xFasterTransformer</td>
        <td>https://github.com/intel/xFasterTransformer</td>
    </tr>
</table>

## 三、IPEX-LLM 部署工具

### 1. IPEX-LLM 简介

IPEX-LLM 是 Intel 推出的一款能使大模型在 CPU 上进行推理的部署工具，旨在提高 LLM 模型的部署效率，降低部署成本。使用 IPEX-LLM 进行模型部署需要经过以下步骤：

- 准备工具、模型及其环境
- 模型加载
- 模型部署推理

下文将详细介绍使用 IPEX-LLM 量化并部署 Qwen2-1.5B-Instruct 步骤。

### 2. 使用 IPEX-LLM 部署 LLM 步骤

#### （0）准备工作
首先进入 Modelscope 社区，在个人主页中选中左侧导航栏中的 `我的Notebook`，在`魔搭平台免费实例`的`PAI-DSW`环境中，使用`方式一`创建一个含以下配置的开发机环境：
- 8核 32GB 
- 预装 ModelScope Library
- 预装镜像 ubuntu22.04-py310-torch2.1.2-tf2.14.0-1.14.0

静待一段时间后，机器初始化完成，出现 `查看Notebook` 按钮，如 `图 1` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/1.png"
    alt="开启开发机" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 1 开启开发机</div>
</div>

点击 `查看Notebook` 按钮，进入 JupyterLab 界面，点击右上角的 `New` 按钮，选择 `Terminal`，进入终端界面。


#### （1）下载 IPEX-LLM 及 Qwen2-1.5B-Instruct 模型

在终端中输入以下命令，下载并解压已经配置好的 IPEX-LLM Jupyter Kernel 环境：

```bash
mkdir ipex
wget https://s3.idzcn.com/ipex-llm/ipex-llm-2.1.0b20240410.tar.gz
tar -zxvf ipex-llm-2.1.0b20240410.tar.gz -C ipex/ && rm ipex-llm-2.1.0b20240410.tar.gz
```

在等待的过程中，我们可以安装 ipykernel，并将 IPEX-LLM 环境添加到 JupyterLab 中，只需运行以下代码：

```bash
pip install ipykernel
ipykernel install --name=ipex
```

同时，可以运行 `baseline1` 中的第二段代码，下载 Qwen2-1.5B-Instruct 模型。

#### （2）加载模型

在 Jupyter 界面中，点击右上角的 `Kernel`，选择 `Change Kernel`，选择 `ipex`，即可加载 IPEX-LLM 环境，如 `图 2` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/2.png"
    alt="加载 IPEX-LLM 环境" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 2 加载 IPEX-LLM 环境</div>
</div>

该环境下，直接运行 `baseline1` 中的剩余代码，进行 Qwen2-1.5B-Instruct 的量化与文件准备。运行结束后，当前目录的文件结构应为 `图 3` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/3.png"
    alt="文件结构" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 3 文件结构</div>
</div>

> 此处提到的量化，即低精度量化（Low Precision Quantization）——将浮点数转换为低位宽的整数（这里是 int4 ），以减少计算资源的需求和提高系统的效率。这种技术在深度学习模型中尤其重要，它可以在硬件上实现快速、低功耗的推理，也可以加快模型加载的速度。
> 
> 通过 IPEX-LLM 中用预训练模型的 int4 量化加载
> 
> ```python
> model = ipex_llm.transformers.AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit='sym_int4', trust_remote_code=True)
> tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
> ```
> 
> 又用模型的保存工具进行保存，即可获得量化后的模型文件。
> 
> ```python
> model.save_low_bit('qwen2chat_int4')
> tokenizer.save_pretrained('qwen2chat_int4')
> ```
> 
> 要想加载不同量化方式得到的量化权重，可参考[官方文档](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/example/CPU/HF-Transformers-AutoModels/Advanced-Quantizations)
> 
> <table>
>     <tr>
>         <th>量化权重</th>
>         <th>说明文档</th>
>     </tr>
>     <tr>
>         <td>GGUF</td>
>         <td>
>              <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF/">文档</a>
>          </td>
>     </tr>
>     <tr>
>         <td>AWQ</td>
>         <td>
>              <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Advanced-Quantizations/AWQ/">文档</a>
>          </td>
>     </tr>
>     <tr>
>         <td>GPTQ</td>
>         <td>
>              <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Advanced-Quantizations/GPTQ/">文档</a>
>          </td>
>     </tr>
> </table>


#### （3）部署推理

LLM 加载完成后，随后便能够执行多种任务，如对话生成、文本生成、文本分类等，我们也有多种多样的方式来进行部署推理，如单次输出、流式输出、Gradio 交互对话、Streamlit 交互对话等。

> Graido 是一个 Python 库，可以快速构建和分享机器学习模型的界面。它支持多种模型，包括分类、检测、分割、生成等，可以在几行代码中构建一个交互式界面，用于与模型进行交互。
> 
> Streamlit 也是一个辅助快速构建机器学习应用的 Python 库，可以在几行代码中构建一个交互式界面，用于与模型进行交互。与 Gradio 不同的是，Streamlit 更加灵活，可以实现更多的功能，如数据可视化、模型训练、模型评估等。

下文将介绍如何在 IPEX-LLM 环境中进行 Qwen2-1.5B-Instruct 的部署推理与交互体验。

##### 单次输出
在终端中输入以下命令，进行 Qwen2-1.5B-Instruct 的部署推理：

```bash
cd /mnt/workspace
conda activate ipex
python3 run.py
```

运行结束后，即可查看 Qwen2-1.5B-Instruct-Int4 对于一个问题的回答结果，如 `图 4` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/4.png"
    alt="回答结果" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 4 单次输出</div>
</div>


##### 流式输出

再在终端中输入以下命令，又能够实现进行 Qwen2-1.5B-Instruct-Int4 在控制台中流式输出，如 `图 5` 所示，生成完毕共花费约 7.79s。

```bash
python3 run_stream.py
```

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/5.png"
    alt="流式输出" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 5 流式输出</div>
</div>

##### Gradio 交互对话

在 Modelscope 开发机中，个人 id 可在 JupyterLab 中的终端窗口名或网页地址中找到，则应当先修改 `run_gradio_stream.py` 中最后一行代码中的根地址为自己的 id，再运行后文的命令。修改如 `图 6` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/6.png"
    alt="修改路由地址" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 6 修改路由地址</div>
</div>

在终端中输入以下命令，安装并启动 Gradio 可视化组件及大模型的，实现 Qwen2-1.5B-Instruct-Int4 在 Gradio 中的与用户的对话。待启动完成后，打开浏览器，输入服务网页链接，便可与模型进行对话，如 `图 7` 所示。

> 对于 modelscope 平台，结合代码中的默认地址，可在浏览器中输入 `https://dsw-gateway-cn-hangzhou.data.aliyun.com/dsw-{id}/proxy/7860/` 进行对话。
> 
> 其中，`{id}` 为个人 id，可在 JupyterLab 中的终端窗口名或网页地址中找到。

```bash
pip install gradio
python3 run_gradio_stream.py
```

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/7.png"
    alt="Gradio 对话" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 7 Gradio 对话</div>
</div>

##### Streamlit 交互对话

在终端中输入以下命令，安装并启动 Streamlit 可视化组件及大模型的，实现 Qwen2-1.5B-Instruct-Int4 在 Streamlit 中的与用户的对话。待启动完成后，打开浏览器，输入服务网页链接，便可与模型进行对话，如 `图 8` 所示。

> 对于 modelscope 平台，结合代码中的默认地址，可在浏览器中输入 `https://dsw-gateway-cn-hangzhou.data.aliyun.com/dsw-{id}/proxy/8501/` 进行对话。
> 
> 其中，`{id}` 为个人 id，可在 JupyterLab 中的终端窗口名或网页地址中找到。

```bash
pip install streamlit
streamlit run run_streamlit_stream.py
```
<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/llm_app_dev_study/8.png"
    alt="Streamlit 对话" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 8 Streamlit 对话</div>
</div>

## 四、总结

本次笔记主要介绍了 Datawhale AI 夏令营 LLM 应用开发实战笔记 Task1 的内容，包括赛题的理解、模型获取、IPEX-LLM 部署工具的使用等。通过学习和实践，我们可以更好地理解模型获取和部署工具的使用方法，提高模型的性能，实现更加准确、高效的应用开发。

将大模型部署到 CPU 上而不是 GPU 上也是一次有趣的尝试，也希望能够通过本次的体验，我能够更好地理解和掌握大模型的部署和优化技术，为未来的深度学习应用开发打下坚实的基础。

## 参考文章

- [从零入门CPU部署大模型&应用开发 - Datawhale AI 夏令营](https://datawhaler.feishu.cn/wiki/SuppwSfFZi5KvYkiffUcygVNnPe)
- [Task1：了解模型获取和IPEX-LLM部署工具 - Datawhale AI 夏令营](https://datawhaler.feishu.cn/wiki/Gka7wVHxziP4fKkgDwXcdiZxnah)

