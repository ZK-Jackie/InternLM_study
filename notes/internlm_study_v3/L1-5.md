<div style="border-bottom: 4px solid black; width: 100%; box-sizing: border-box; text-align: center; padding-top: 0.1rem;" align="center">
    <h1>书生大模型实战营「第3期」学员笔记<br/><span>基础岛 - XTuner 微调个人小助手认知</span></h1>
</div>
<div style="text-align: center;" align="center">
    笔记记录人：ZK-Jackie&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;笔记记录时间：2024.7.30
</div>

## 目录

- [1 微调理论](#1-微调理论)
    - [1.1 微调 fine-tune 简介](#11-微调-fine-tune-简介)
    - [1.2 XTuner 简介](#12-xtuner-简介)
- [2 实操任务](#2-实操任务)
    - [2.1 基础任务](#21-基础任务)
    - [2.2 进阶任务](#22-进阶任务)
- [3 总结](#3-总结)
- [参考资料](#参考资料)
- [备注](#备注)

## 1 微调理论

大语言模型是在海量文本内容上，以无监督或半监督的方式进行训练的模型，它的训练数据集通常包含了大量的文本数据，如维基百科、新闻、书籍、网页等。这些模型在训练过程中，通过学习文本数据中的统计规律，从而学习到了丰富的语言知识，包括语法、语义、逻辑等。

但是，由于训练数据的局限性，大语言模型在实际应用中还存在一些局限性，如知识时效性受限、专业能力有限、定制化成本高等问题。为了解决这些问题，令他能够有更好的应用价值，当下也有多种基本思路解决它，如 Prompt Engineering， Finetune（微调）和 RAG（检索增强生成）。

在前面的学习中，我们已经了解了如何编写 Prompt 让 LLM 正确执行任务，了解了如何利用 LlamaIndex 和 LangChain 等框架使用 RAG 范式解决这一问题，本节课程中我们将了解如何通过 Finetune（微调）的方式，来解决大语言模型的局限性。

### 1.1 微调 fine-tune 简介

大语言模型的微调（Fine-tuning）是一种常用的**深度学习技术**，它是在预训练模型的基础上，将模型中一些层的权重参数进行微调，以适应新的数据集或任务。微调是一种基于预训练模型，通过少量的调整（fine-tune）来适应新的任务或数据的方法。

由于大型语言模型的参数规模巨大，且规模日益增大，导致模型的训练和微调成本高昂，直接训练需要耗费大量计算资源和费用。近年来，如何高效地对大模型进行微调成为了研究热点，而 LoRA 和 QLoRA 两种微调技术因其高效性和实用性受到了广泛关注。它们的基本原理是在不改变原有模型权重的情况下，通过外部添加少量新参数来进行微调，这降低了模型的存储需求，也降低了计算成本，实现了对大模型的快速适应，同时保持了模型性能。

一般情况下，微调一个大模型有以下几个步骤：

- **准备数据集**：准备好微调所需的数据集，数据集的质量和数量对微调效果有很大的影响。
- **选择模型**：选择一个预训练模型作为基础模型，根据任务的不同选择不同的模型。
- **微调模型**：在预训练模型的基础上，将模型中一些层的权重参数进行微调，以适应新的数据集或任务。

下文也将进行这些工作。

### 1.2 XTuner 简介

XTuner 是一个大语言模型&多模态模型微调工具箱，由 MMRazor 和 MMDeploy 联合开发。XTuner 以配置文件的形式封装了大部分微调场景，能让零基础的非专业人员也能一键开始微调。对于 7B 参数量的 LLM，微调所需的最小显存仅为 8GB，支持在消费级显卡和 colab 等设备上进行微调。

XTuner 是书生浦语全链路开源体系中的一部分，适配多种生态，支持多种微调算法，且简单上手，后文将使用该微调工具箱进行微调任务。

## 2 实操任务

### 2.1 基础任务

> - 任务描述：使用 XTuner 微调 InternLM2-Chat-1.8B 实现自己的小助手认知，如下图所示（图中的`伍鲜同志`需替换成自己的昵称），记录复现过程并截图。
> - 实现步骤：
>   1. 准备 XTuner 运行环境
>   2. 准备微调数据
>   3. 准备微调配置
>   4. 开展微调
>   5. 观察结果
> - 硬件环境：30% A100 * 1
> - 软件环境：Cuda 12.2-Conda

#### 2.1.1 准备 XTuner 运行环境

在 InterStudio 开发环境中，创建并激活一个新的 Conda 环境用于执行微调任务，这里将其命名为 `internlm`。随后在该环境中安装 XTuner 工具箱，即运行以下代码：

```bash
conda create -n internlm python=3.10
conda activate internlm
pip install xtuner[deepspeed]
```

本次安装的是带有 DeepSpeed 的 XTuner 版本，DeepSpeed 是一个用于训练大型模型的深度学习库，它提供了一种高效的分布式训练方法，能够在大规模 GPU 集群上训练大型模型。简而言之，微调过程中使用上 DeepSpeed 可以提高训练速度。

#### 2.1.2 准备微调数据

接下来，我们将准备微调材料，包括微调数据集、微调配置文件等。本次我即将为它提高一个名为 `ZK-Jackie` 的小助手认知，让它能够以“`ZK-Jackie` 的小助手”的名义回答用户的问题。依照 [XTuner 官方文档对数据集的标准组织形式](https://github.com/InternLM/xtuner/blob/main/docs/en/user_guides/dataset_prepare.md)，我准备的数据集中的文本内容为：

```plaintext
[
    {
        "conversation": [
            {
                "input": "请做一下自我介绍",
                "output": "我是ZK-Jackie的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"
            }
        ]
    },
    {
        "conversation": [
            {
                "input": "请介绍一下你自己",
                "output": "我是ZK-Jackie的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"
            }
        ]
    },
    {
        "conversation": [
            {
                "input": "说说你自己",
                "output": "我是ZK-Jackie的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"
            }
        ]
    }
]
```

由于微调所需的数据量必须是比较大的，则我们还应当使用相关工具复制上面的内容以生成更多的数据集，以提高微调的效果。仅需编写以下代码完成这一任务即可：

```python
# /root/Tutorial/L1/xtuner/build_dataset.py
import json
scale = 2000
data = [{"conversation": [{"input": "请做一下自我介绍","output": "我是ZK-Jackie的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"}]},
    {"conversation": [{ "input": "请介绍一下你自己","output": "我是ZK-Jackie的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"}]},
    {"conversation": [{"input": "说说你自己","output": "我是ZK-Jackie的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"}]}]
for i in range(scale):
    for j in range(3):
        data.append(data[j])
with open("/root/Tutorial/L1/xtuner/myDataset.json", "w") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)
```

运行上面的代码，收获了千余条数据后，我们就已经将数据集准备好了，接下来则该准备相应的配置文件。

#### 2.1.3 准备微调配置

参考 [XTuner 的官方文档中对其他数据集的微调配置文件示例]()，我们可以了解到对于自定义的数据集的微调配置的写法，则我们可以复制原有的配置文件，并对相应行进行修改：

```bash
cd /root/Tutorial/L1/xtuner
xtuner copy-cfg internlm2_chat_1_8b_qlora_alpaca_e3 . 
```

```bash
# 修改import部分
- 15 from xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory
+ 15 from xtuner.dataset.map_fns import template_map_fn_factory

# 修改模型为本地路径
- 27 pretrained_model_name_or_path = 'internlm/internlm2-chat-1_8b'
+ 27 pretrained_model_name_or_path = '/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b'

# 修改训练数据为数据集路径和变量名字
- 31 alpaca_en_path = 'tatsu-lab/alpaca'
+ 31 data_path = 'myDataset.json'

# 修改 evaluation_inputs 评估文本
- 60  '请给我介绍五个上海的景点', 'Please tell me five scenic spots in Shanghai'
+ 60  '请介绍一下你自己', 'Introduce yourself'

# 修改 alpaca_en 对象中的 dataset 参数
- 102  dataset=dict(type=load_dataset, path=alpaca_en_path),
+ 102  dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),

- 105  dataset_map_fn=alpaca_map_fn,
+ 105  dataset_map_fn=None,
```

#### 2.1.4 开展微调

在准备好数据集和配置文件后，使用 XTuner 工具箱进行微调，在终端中运行以下代码（不要忘记 `./` 符号！），微调便会开始，如 `图 1` 所示。

```bash
xtuner train ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py --deepspeed deepspeed_zero2
```

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/1.png"
    alt="ft-start" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 1 微调开始</div>
    <br/>
</div>


#### 2.1.5 观察结果

由于我提供的数据量较少，等待一段时间后，微调就完成了，获得了微调获取的以 pth 结尾的 pytorch 模型文件，其文件夹结构如下 `图 2` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/2.png"
    alt="ft-result" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 2 微调输出文件</div>
    <br/>
</div>

其中也记录了微调的日志文件、启动配置、检查点等信息，都可以帮助我们保护微调过程恢复，了解微调过程及结果。

由于我们所使用的模型都是 HuggingFace 模型，则还需要对其进行一定处理，以便后续使用。XTuner 中已经集成了有关模型转换、合并、推理、对话等功能，只需运行以下代码，将 pth 模型转换为 HuggingFace Adapter，并直接借助 Adapter 对话。

```bash
mkdir hf    # 创建一个文件夹用于存放转换后的模型
xtuner convert pth_to_hf ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py ./work_dirs/internlm2_chat_1_8b_qlora_alpaca_e3_copy/iter_288.pth ./hf  # 转换模型，注意iter根据实际情况一般选最大的
xtuner chat /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --adapter ./hf --prompt-template internlm2_chat   # 使用 Adapter 对话
```

随后，我们可以直接在控制台中输入问题，查看微调后获得的 Adapter 是否能够有效作用。我们本次微调的目的是修正其自我认知问题，则我们在控制台中输入“请介绍一下你自己”，以校验结果。如 `图 3` 所示，它已经能够按照我们的预期回答问题了。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/3.png"
    alt="ft-chat" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 3 微调后对话</div>
    <br/>
</div>

### 2.2 进阶任务

> - 任务描述：将自我认知的模型上传到 OpenXLab，并将应用部署到 OpenXLab
> - 实现步骤：
>   1. 合并模型
>   2. 在 OpenXLab 上创建模型空间并上传模型
>   3. 使用 LMDeploy 在 OpenXLab 上部署模型

#### 2.2.1 合并模型

基础任务中，我们已经获得了微调后的 HuggingFace Adapter 模型，接下来我们将对其进行合并，以便在 OpenXLab 上使用。在终端中运行以下代码，将 Adapter 模型合并为一个文件。

```bash
mkdir hf-merged   # 创建一个文件夹用于存放合并后的模型
xtuner convert merge /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b ./hf ./hf-merged --max-shard-size 2GB    # 合并模型并限制每个文件大小
```

合并完成后，`hf-merged` 文件夹下的文件即为基于 InternLM2-Chat-1.8B 的，经过了微调、权重融合的模型文件，可以直接上传到 OpenXLab 上。

#### 2.2.2 在 OpenXLab 上创建模型空间并上传模型

进入 OpenXLab 官网，登录账号，在页面右上方可以选择“创建模型”，如下 `图 4` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/4.png"
    alt="create-model" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 4 创建模型</div>
    <br/>
</div>

在弹出的页面中选择相关介绍信息，就可以完成模型仓库的创建，下 `图 5` 即为创建完成后的页面。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/5.png"
    alt="model-repo" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 5 模型仓库</div>
    <br/>
</div>

正式将文件上传到仓库前，还需配置 OpenXLab 的 Git 访问密钥。点击页面右上方头像并选择“密钥管理”子菜单，在弹出的页面的“Git 访问令牌”选项页面下，选择“添加令牌”（已有“可写”令牌可跳过创建），填写令牌名称后，选择令牌权限为“可写”，点击“创建令牌”即可生成令牌。总体步骤如下 `图 6` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/6.png"
    alt="create-token" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 6 生成令牌</div>
    <br/>
</div>

生成令牌后，回到 InternStudio 开发机，在已经合并好的模型文件夹下，运行以下代码，安装 git-lfs 并初始化本地 git 仓库，为上传做准备。

```bash
cd hf-merged
apt update && apt install git-lfs
git init
```

随后，回到 OpenXLab 的模型仓库页面，复制仓库的 Git 地址，如下 `图 7` 。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/8.png"
    alt="copy-git" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 7 复制 Git 地址</div>
    <br/>
</div>

初始化 git-lfs，为当前 git 仓库设置个人身份信息；随后与远程仓库地址同步，远程仓库地址应当结合前文生成的令牌、个人 OpenXLab 仓库地址、前文中获取到的拥有“可写权限的 git token”以填补空缺信息；在合并好的模型文件夹下运行以下代码，拉取仓库并上传模型文件。

```bash
git lfs install   # 初始化 git-lfs
git config user.email "{your_email}"  # 建议和 OpenXLab 的账号信息一致
git config user.name "{your_name}"    # 要和 OpenXLab 的账号信息一致
git checkout -b main    # 必须创建并切换到 main 分支
git pull https://{your_username}:{your_git_token}@code.openxlab.org.cn/{your_username}/{your_repo_addr}.git --allow-unrelated-histories  # 拉取仓库
git remote add origin https://{your_username}:{your_git_token}@code.openxlab.org.cn/{your_username}/{your_repo_addr}.git --allow-unrelated-histories
git add .   # 添加所有文件
git commit -m "Initial Commit"  # 提交
git push origin main   # 推送
```

按顺序填补信息，提交信息后，需要在控制台或 VSCode 顶端的框中多次输入 OpenXLab 的用户名和 git token。一段时间后，上传完成，回到 OpenXLab 的模型仓库页面，即可看到已经上传的模型文件，如下 `图 8` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/9.png"
    alt="upload-model" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 8 上传模型</div>
    <br/>
</div>

#### 2.2.3 使用 LMDeploy 在 OpenXLab 上部署模型

模型上传完成后，我们再编写以下文件，使用 LMDeploy 部署我们微调过后的模型，利用 Gradio 实现与我们模型的对话，用于准备将我们刚上传的模型部署到 OpenXLab 上。

注意代码中要将相关信息修改为自己个人的信息，如 git token、模型地址、用户名等。但启动端口必须设置为 7860.

```python
# app.py
import os
from lmdeploy.serve.gradio.turbomind_coupled import run_local
from lmdeploy.messages import TurbomindEngineConfig
# 获取个人 token
git_token = os.getenv('GIT_READ_TOKEN')
# 下载微调后的模型
base_path = os.path.dirname(__file__)
model_path = f'{base_path}/ft-model'
os.system(f'git clone https://ZK-Jackie:{git_token}@code.openxlab.org.cn/ZK-Jackie/my-internlm.git {model_path}')
os.system(f'cd {model_path} && git lfs pull')
# 部署模型
run_local(model_path, server_port=7860)
```

```text
# requirements.txt
lmdeploy[serve]==0.5.0
```

```text
# packages.txt
git
git-lfs
```

将上述文件上传至自己的 GitHub 仓库中，并都放置在仓库最外一层目录，如[我的仓库](http://github.com/ZK-Jackie/my-internlm)所示。接着，我们点击 OpenXLab 右上方创建按钮，选择“创建应用”，如下 `图 9` 所示。 

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/10.png"
    alt="create-app" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 9 创建应用</div>
    <br/>
</div>

在弹出的页面中，选择 Gradio 作为应用类型，并点击“开始创建”，如下 `图 10` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/11.png"
    alt="select-gradio" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 10 选择 Gradio</div>
    <br/>
</div>

在下一步弹出的页面中，填写应用的相关信息和自己放置了将要部署的代码的 GitHub 仓库，并在高级配置中添加一个名为“GIT_READ_TOKEN”的环境变量。该 token 的生成可参考前文生成令牌的步骤。如下 `图 11`  `图 12` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/12.png"
    alt="config-app" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 11 配置应用</div>
    <br/>
</div>

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/13.png"
    alt="advanced-config" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 12 高级配置</div>
    <br/>
</div>

最后点击页面底部的“创建应用”按钮，平台边将会从指定的 GitHub 仓库和分支中拉取代码，安装依赖，开始构建、执行代码，如下 `图 13` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/14.png"
    alt="deploy-app" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 13 部署应用</div>
    <br/>
</div>

这一过程将持续约 5 分钟。一般情况下，申请 4vCPU 16GB RAM Nvidia vGPU 8GB 的资源即可满足部署需求。若构建失败，且日志并未出现任何明显错误信息，那大概率是硬件资源不足以支撑起应用运行，可[点击此处](https://openxlab.org.cn/apps/apply-hardware?creator=ZK-Jackie)进行资源申请。

解决完一切问题后，我们便能够在 OpenXLab 使用我们的应用。OpenXLab 也提供多种功能服务有待我们探索~ 应用部署完成和运行对话效果如下 `图 14` `图 15` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/15.png"
    alt="deploy-success" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 14 部署应用成功</div>
    <br/>
</div>

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-5/16.png"
    alt="app-dialog" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 15 应用运行对话</div>
    <br/>
</div>


至此，我们已经完成了将自我认知的模型上传到 OpenXLab，并将应用部署到 OpenXLab 的任务，整个过程中我们使用了 XTuner 工具箱进行微调，使用 LMDeploy 部署模型，实现了自己的小助手认知。

## 3 总结

本节课程中，我们了解了微调的基本概念和 XTuner 工具箱的使用，通过 XTuner 工具箱对 InternLM2-Chat-1.8B 模型进行了微调，实现了自己的小助手认知。接着，我们将微调后的模型上传到 OpenXLab，并将应用部署到 OpenXLab，实现了与模型的对话。整个过程中，我们使用了 XTuner 工具箱、LMDeploy 工具箱，实现了自己的小助手认知，为后续的工作打下了基础。

## 参考资料

- [XTuner - GitHub](https://github.com/InternLM/xtuner)
- [XTuner - 官方文档](https://xtuner.readthedocs.io)
- [OpenXLab浦源](https://openxlab.org.cn/)
- [Gradio 应用 | OpenXLab浦源 - 文档中心](https://openxlab.org.cn/docs/apps/Gradio%E5%BA%94%E7%94%A8.html)
- [LMDeploy 部署 Gradio 应用 - GitHub](https://github.com/InternLM/lmdeploy/blob/main/docs/zh_cn/serving/gradio.md)

## 备注

由于本次实操任务开始及完成较早，部分内容可能与最新的实操任务要求和执行步骤有所出入，仅供参考。