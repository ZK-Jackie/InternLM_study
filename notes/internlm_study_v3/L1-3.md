<div style="border-bottom: 4px solid black; width: 100%; box-sizing: border-box; text-align: center; padding-top: 0.1rem;" align="center">
    <h1>书生大模型实战营「第3期」学员笔记<br/><span>基础岛 - 浦语提示词工程实践</span></h1>
</div>
<div style="text-align: center;" align="center">
    笔记记录人：ZK-Jackie&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;笔记记录时间：2024.7.26
</div>

## 目录

- [1 提示词与提示词工程](#1-提示词与提示词工程)
  - [1.1 提示词](#11-提示词)
  - [1.2 提示词工程](#12-提示词工程)
  - [1.3 LangGPT](#13-langgpt)
- [2 实操任务](#2-实操任务)
  - [2.1 基础任务](#21-基础任务)
    - [2.1.1 向 LangGPT 提交提示词请求](#211-向-langgpt-提交提示词请求)
    - [2.1.2 向 internlm2-chat-1.8b 提交提示词请求](#212-向-internlm2-chat-18b-提交提示词请求)
  - [2.2 进阶任务](#22-进阶任务)
    - [2.2.1 GSM8k 任务](#221-gsm8k-任务)
    - [2.2.2 TruthfulQA 任务](#222-truthfulqa-任务)
- [3 总结](#3-总结)
- [参考资料](#参考资料)
- [附录](#附录)
- [备注](#备注)

## 1 提示词与提示词工程

### 1.1 提示词

Prompt 即提示词，**Prompt 最初是 NLP（自然语言处理）研究者为下游任务设计出来的一种任务专属的输入模板，类似于一种任务（例如：分类，聚类等）会对应一种
Prompt。**

在 ChatGPT 推出并获得大量应用之后，Prompt 开始被推广为给大模型的所有输入，即用户向LLM输入Prompt，LLM返回 Completion 内容。

在使用 ChatGPT API 时，可以设置两种 Prompt：

- System Prompt:**该种 Prompt 内容会在整个会话过程中持久地影响模型的回复，且相比于普通 Prompt 具有更高的重要性**
- User Prompt:**该种 Prompt 会在每次对话中被模型使用，用于指导模型生成回复**

我们一般设置 System Prompt 来对模型进行一些**初始化设定**。在通过 System Prompt 设定好模型的人设或是初始设置后，我们通过
User Prompt 给出模型需要遵循的指令或提出问题。

随着大模型技术的发展，Prompt 的类别和形式也在不断丰富和变化，对于支持工具调用的大模型，我们可以向它传入有关工具的
Prompt，让模型帮助我们完成一些工具操作；对于支持音频输入的大模型，我们可以向它传入音频文件的 Prompt，让模型帮助我们完成音频内容的转写。

只有正确的 Prompt 设计和使用，才能大模型更好地完成我们的需求，Prompt 的设计和使用成为了大模型应用中的重要环节。

### 1.2 提示词工程

Prompt Engineering 即提示词工程，是指**设计和使用 Prompt 的过程**。Prompt Engineering 是一个**复杂的过程**
，需要我们根据具体的任务和模型特性来设计和使用 Prompt。

一般来说，Prompt 的设计需要遵循以下几个原则：

- 编写清晰、具体的指令
- 按一定的逻辑顺序组织 Prompt
- 保持简洁，避免Prompt过长

Prompt Engineering 过程中，大致包括以下几个步骤：

- 初始化：在小样本中调整 Prompt ，尝试使其在这些样本上起效。
- 解决 Bad Case：进行进一步测试时，可能会遇到一些棘手的例子，这些例子无法通过 Prompt
  或者算法解决。在这种情况下，可以将这些额外的几个例子添加到正在测试的集合（开发集迭代）中，有机地添加其他难以处理的例子。
- 自动评估：开发集不断扩大至不便测试的规模时，可开始开发一些用于衡量这些小样本集性能的指标，例如平均准确度。

不断地重复进行上述过程，直至我们构建出一个高效的 Prompt，使得模型能够更好地完成我们的需求。

有关于更多 Prompt 和 Prompt Engineering 的内容，可以参考[参考资料](#参考资料)中的相关链接。

### 1.3 LangGPT

LangGPT 是一个帮助用户编写高质量提示词的工具，并具有一套模块化、标准化的提示词编写方法论——结构化提示词。其开发团队的愿景是每一个人都能够写出高质量的提示词，更好地利用大模型的能力。

LangGPT 团队分别在 OpenAI 商店和 月之暗面 Kimi 中发布了 LangGPT 辅助提示词编写工具，本次实操任务中，我们将使用 LangGPT
的工具，优化提示词，使LLM输出正确结果。

## 2 实操任务

### 2.1 基础任务

> - 背景问题：近期相关研究发现，LLM在对比浮点数字时表现不佳，经验证，internlm2-chat-1.8b (internlm2-chat-7b)也存在这一问题，例如认为
    13.8<13.11。
> - 任务要求：利用 LangGPT 优化提示词，使LLM输出正确结果。
> - 实现步骤：
>     1. 向 LangGPT 提交提示词请求
>     2. 向 internlm2-chat-1.8b 提交提示词请求
>     3. 优化提示词，使LLM输出正确结果
>     4. 重复步骤 1-3，直至LLM输出正确结果，记录优化过程并截图
> - 硬件环境：10% A100 * 1
> - 软件环境：Cuda12.2-Conda

#### 2.1.1 向 LangGPT 提交提示词请求

本次任务主要需解决的是大语言模型对比浮点数字时表现不佳的问题，因此我们需要向 LangGPT 提交这一类提示词请求。前往 LangGPT
的工具发布页面，向 LangGPT 提交提示词请求，我提交的提示词和请求如下：

```plaintext
生成一个能够精准比较浮点数字的助手提示词。
```

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-3/1.png"
    alt="prompt-require" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 1 提交提示词请求</div>
    <br/>
</div>

即 LangGPT 助手的回答如下，我们便能够直接向 InternLM2-Chat-1.8B 发送该提示词请求。

```plaintext
# Role: 浮点数字比较助手

## Profile
- author: LangGPT 
- version: 1.0
- language: 中文/英文
- description: 该助手用于精确比较浮点数值，帮助用户在各种场景下进行数值比较操作。

## Skills
1. 精确比较两个浮点数，考虑到浮点数的精度问题。
2. 提供接近零的容差范围，避免因浮点数的精度问题导致错误判断。
3. 支持不同的浮点数比较方法，包括直接比较、指定容差比较等。
4. 能够处理科学计数法表示的浮点数。
5. 对异常情况（如NaN、无穷大）进行合理处理。

## Rules
1. 使用绝对值差值小于指定容差范围的方法进行浮点数比较。
2. 默认容差范围为1e-9，但允许用户自定义该值。
3. 当两个浮点数的绝对值差值小于容差范围时，判定两个数相等。
4. 对于特殊浮点数值（如NaN、正负无穷大），按照IEEE 754标准进行处理。

## Workflows
1. 接收用户输入的两个浮点数值和（可选的）自定义容差范围。
2. 计算两个浮点数的绝对值差值。
3. 将差值与容差范围进行比较，判断两个数是否相等。
4. 返回比较结果，并说明判断依据。
5. 若输入的浮点数包含特殊值（如NaN、无穷大），则按照IEEE 754标准进行处理并返回结果。

## Init
1. 欢迎用户使用浮点数字比较助手。
2. 询问用户输入的两个浮点数值及（可选的）自定义容差范围。
3. 根据用户输入进行比较，并返回结果。
```

#### 2.1.2 向 internlm2-chat-1.8b 提交提示词请求

按照[上一次笔记](./L1-2.md)中的步骤，在相同开发环境下使用 LMDeploy 部署带 WebUI 的
InternLM2-Chat-1.8B，并以用户身份直接向它提交提示词请求，如下 `图 2` 所示：

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-3/2.png"
    alt="prompt-submit" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 2 提交提示词请求</div>
    <br/>
</div>

可以看到，InternLM2-Chat-1.8B 已经成功接收到了我们的提示词请求，接下来我们向它询问一些浮点数比较问题，看看它的回答是否正确。

多轮对话的记录如下表所示：

| 用户输入               | InternLM2-Chat-1.8B 回答                                                                                        |
|--------------------|---------------------------------------------------------------------------------------------------------------|
| 13.8 和 13.11 哪个更大？ | 根据浮点数的精度，13.8 和 13.11 是两个非常接近的浮点数，它们之间的绝对值差值约为 0.001。因此，13.8 比 13.11 更大。如果您需要更精确的比较，请告诉我您希望使用哪种精度，我会尽力满足您的需求。 |
| 13.8 和 13.11 哪个更小？ | 根据浮点数的精度，13.8 和 13.11 是两个非常接近的浮点数，它们之间的绝对值差值约为 0.001。因此，13.11 比 13.8 更小。如果您需要更精确的比较，请告诉我您希望使用哪种精度，我会尽力满足您的需求。 |
| 3.8 和 3.11 哪个大？    | 根据浮点数的精度，3.8 和 3.11 是两个非常接近的浮点数，它们之间的绝对值差值约为 0.001。因此，3.8 比 3.11 更大。如果您需要更精确的比较，请告诉我您希望使用哪种精度，我会尽力满足您的需求。     |

在一开始没有提交提示词的请求时，InternLM2-Chat-1.8B 的回答内容如下表所示：

| 用户输入               | InternLM2-Chat-1.8B 回答                         |
|--------------------|------------------------------------------------|
| 13.8 和 13.11 哪个更大？ | 13.8 和 13.11 都是小数，但它们的数值大小不同。13.8 是大于 13.11 的。 |
| 13.8 和 13.11 哪个更小？ | 13.8 比 13.11 更小。                               |
| 3.8 和 3.11 哪个大？    | 3.8 比 3.11 更大。                                 |

通过二者对比，可以看到，提交了提示词请求后，InternLM2-Chat-1.8B 的回答效果更加准确，能够更好地执行浮点数值比较问题。相关图片记录见[附录-基础任务材料](#附录)。




### 2.2 进阶任务

> - 任务描述：选择任务，基于 LangGPT 格式编写系统提示词，使用 OpenCompass 让 Internlm2.5-chat-7b 完成该任务。
> - 实现步骤：
>   1. 准备 OpenCompass 评测环境
>   2. 编写系统提示词并使用 OpenCompass 进行评测
>   3. 优化提示词，使 Internlm2.5-chat-7b 完成该任务
> - 硬件环境：30% A100 * 1

通过基础任务的实践，我们已经了解到了 LangGPT 的结构化提示词编写的特点和优势，下文我将尝试按照 LangGPT 结构化 Prompt
的格式编写提示词，让 Internlm2.5-chat-7b 能够更好地完成 GSM8k 和 TruthfulQA 任务。

#### 2.2.1 GSM8k 任务

根据 OpenAI
研究者的研究，尽管随着大模型的不断发展和参数量的激增，大模型在数学推理任务上的表现也在不断提升，但在多步骤数学推理问题上，大模型的表现仍然不尽如人意。他们发现大模型在题解过程中发生错误时，自回归模型并没有机制来纠正自身错误，导致后续的推理过程中出现的错误正反馈式激增，最终导致推理结果是不正确的。

##### 2.2.1.1 编写第一版提示词

GSM8K 是由人类问题编写者创建的，含 8.5K 高质量小学数学问题，其中主要是多样化的小学数学单词问题，以衡量模型解决多步骤数学推理问题的能力。以
**提高模型的按步骤解题和结果反思修正能力**为目的，我们可以按照 LangGPT 提供的格式编写提示词，如下：

```plaintext
# Role: Grade Math Solver

## Profile
- author: LangGPT
- version: 1.0
- language: English
- description: An specialist of math solving, which aims to correctly solve multi-step math reasoning problems.

## Skills
1. Solve math problems.
2. Support multi-step math reasoning.
3. Provide step-by-step solutions.
4. Think and correct the results in the reasoning process.
5. Always to give the correct answer.

## Rules
1. Solve the math problem step by step.
2. Always check the correctness of each step.
3. Always follow the problem and conditions.
3. Correct the results in the reasoning process.

## Workflows
1. Receive the math problem.
2. Analyze the math problem and conditions.
3. Solve the math problem step by step.
4. Return the final result.
```

##### 2.2.1.2 安装 OpenCompass 及目标测评数据集

完成提示词的编写后，接下来则是使用 OpenCompass 进行评测，让 Internlm2.5-chat-7b 完成 GSM8k 任务。

首先需要在当前 Conda 环境中安装好 OpenCompass，推荐从 GitHub 中拉取其源码进行安装，仅需在开发机中断环境中输入以下命令（假定你的常用 Conda 环境名为 internlm）：

```bash
conda activate internlm
git clone https://github.com/open-compass/opencompass.git
cd opencompass
pip install -e .
```

随后，我们需要从 GitHub 中拉取 OpenAI 开源的 GSM8k 数据集，以便进行评测。数据集 jsonl 文件具体路径可自选，但必须在运行 OpenCompass 时的终端路径下的 data/gsm8k 目录下，以便 OpenCompass 能够正确读取数据集。此处我选择安装在 opencompass 的源码目录下，后续也将在该目录下运行代码。输入以下命令拉取 GSM8k 数据集并放入指定文件夹中：

```bash
cd /root/opencompass/
mkdir -p data/gsm8k
git clone https://github.com/openai/grade-school-math.git
cp grade-school-math/grade-school-math/data/train.jsonl data/gsm8k/
cp grade-school-math/grade-school-math/data/test.jsonl data/gsm8k/
```

##### 2.2.1.3 配置评测任务

根据要求，我们需要指定聊天对话的系统提示词为我们刚编写好的提示词，则需要修改数据记得聊天模板配置。在阅读官方文档，[学习 OpenCompass 的配置方法](https://opencompass.readthedocs.io/zh-cn/latest/user_guides/config.html)后，再结合 [OpenCompass 源码目录下的评测启动配置文件](https://github.com/open-compass/opencompass/blob/main/configs/eval_chat_demo.py)，我们可以很容易地依照 OpenCompass 数据集测试配置文件 `opencompass/configs/datasets/gsm8k/gsm8k_gen_1d7fe4.py` 来自定义我们自己的测试配置文件。此处我选择直接修改源码，并主要修改其中的 `prompt_template` 部分和 `gsm8k_datasets` 部分，添加上我们编写的 System Prompt 文本，修改我们下载的数据集的实际路径。在我的环境中修改完成后，整个文件的内容如下：

```python
# opencompass/opencompass/configs/datasets/gsm8k/gsm8k_gen_1d7fe4.py
from opencompass.openicl.icl_prompt_template import PromptTemplate
from opencompass.openicl.icl_retriever import ZeroRetriever
from opencompass.openicl.icl_inferencer import GenInferencer
from opencompass.datasets import GSM8KDataset, gsm8k_postprocess, gsm8k_dataset_postprocess, Gsm8kEvaluator

gsm8k_reader_cfg = dict(input_columns=['question'], output_column='answer')

gsm8k_infer_cfg = dict(
    prompt_template=dict(
        type=PromptTemplate,
        template=dict(
            begin=[
                dict(role='SYSTEM', fallback_role='HUMAN', prompt='# Role: Grade Math Solver\n\n## Profile\n- author: LangGPT\n- version: 1.0\n- language: English\n- description: An specialist of math solving, which aims to correctly solve multi-step math reasoning problems.\n\n## Skills\n1. Solve math problems.\n2. Support multi-step math reasoning.\n3. Provide step-by-step solutions.\n4. Think and correct the results in the reasoning process.\n5. Always to give the correct answer.\n\n## Rules\n1. Solve the math problem step by step.\n2. Always check the correctness of each step.\n3. Always follow the problem and conditions.\n3. Correct the results in the reasoning process.\n\n## Workflows\n1. Receive the math problem.\n2. Analyze the math problem and conditions.\n3. Solve the math problem step by step.\n4. Return the final result.'),
            ],
            round=[
                dict(role='HUMAN', prompt="Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\nLet's think step by step\nAnswer:"),
                dict(role='BOT', prompt='Angelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.\nFor the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.\nAngelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.\nHowever, they need to include time for breaks and lunch. Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.\nThey also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.\nAnd they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.\nSo Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.\nThey want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75\nThey will need to plan to study 4 days to allow for all the time they need.\nThe answer is 4\n'),
                dict(role='HUMAN', prompt="Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.  Their opponents score double the 2 pointers but half the 3 pointers and free throws.  What's the total number of points scored by both teams added together?\nLet's think step by step\nAnswer:"),
                dict(role='BOT', prompt="Mark's team scores 25 2 pointers, meaning they scored 25*2= 50 points in 2 pointers.\nHis team also scores 6 3 pointers, meaning they scored 8*3= 24 points in 3 pointers\nThey scored 10 free throws, and free throws count as one point so they scored 10*1=10 points in free throws.\nAll together his team scored 50+24+10= 84 points\nMark's opponents scored double his team's number of 2 pointers, meaning they scored 50*2=100 points in 2 pointers.\nHis opponents scored half his team's number of 3 pointers, meaning they scored 24/2= 12 points in 3 pointers.\nThey also scored half Mark's team's points in free throws, meaning they scored 10/2=5 points in free throws.\nAll together Mark's opponents scored 100+12+5=117 points\nThe total score for the game is both team's scores added together, so it is 84+117=201 points\nThe answer is 201\n"),
                dict(role='HUMAN', prompt="Question: Bella has two times as many marbles as frisbees. She also has 20 more frisbees than deck cards. If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles?\nLet's think step by step\nAnswer:"),
                dict(role='BOT', prompt="When Bella buys 2/5 times more marbles, she'll have increased the number of marbles by 2/5*60 = 24\nThe total number of marbles she'll have is 60+24 = 84\nIf Bella currently has 60 marbles, and she has two times as many marbles as frisbees, she has 60/2 = 30 frisbees.\nIf Bella buys 2/5 times more frisbees, she'll have 2/5*30 = 12 more frisbees.\nThe total number of frisbees she'll have will increase to 30+12 = 42\nBella also has 20 more frisbees than deck cards, meaning she has 30-20 = 10 deck cards\nIf she buys 2/5 times more deck cards, she'll have 2/5*10 = 4 more deck cards.\nThe total number of deck cards she'll have is 10+4 = 14\nTogether, Bella will have a total of 14+42+84 = 140 items\nThe answer is 140\n"),
                dict(role='HUMAN', prompt="Question: A group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each fruit in the fourth basket. How many fruits are there?\nLet's think step by step\nAnswer:"),
                dict(role='BOT', prompt='For the first three baskets, the number of apples and oranges in one basket is 9+15=24\nIn total, together with bananas, the number of fruits in one basket is 24+14=38 for the first three baskets.\nSince there are three baskets each having 38 fruits, there are 3*38=114 fruits in the first three baskets.\nThe number of apples in the fourth basket is 9-2=7\nThere are also 15-2=13 oranges in the fourth basket\nThe combined number of oranges and apples in the fourth basket is 13+7=20\nThe fourth basket also contains 14-2=12 bananas.\nIn total, the fourth basket has 20+12=32 fruits.\nThe four baskets together have 32+114=146 fruits.\nThe answer is 146\n'),
                dict(role='HUMAN', prompt="Question: {question}\nLet's think step by step\nAnswer:"),
            ],
        )),
    retriever=dict(type=ZeroRetriever),
    inferencer=dict(type=GenInferencer, max_out_len=512))

gsm8k_eval_cfg = dict(evaluator=dict(type=Gsm8kEvaluator),
                      pred_postprocessor=dict(type=gsm8k_postprocess),
                      dataset_postprocessor=dict(type=gsm8k_dataset_postprocess))

gsm8k_datasets = [
    dict(
        abbr='gsm8k',
        type=GSM8KDataset,
        path='./data/gsm8k',
        reader_cfg=gsm8k_reader_cfg,
        infer_cfg=gsm8k_infer_cfg,
        eval_cfg=gsm8k_eval_cfg)
]
```

若想校验修改结果，还可使用 OpenCompass 提供的 python 工具 `prompt_viewer` 进行测试，只需在终端中输入以下代码：

```bash
python tools/prompt_viewer.py configs/datasets/gsm8k/gsm8k_gen_1d7fe4.py -n -a -p gsm8k
```

python tools/prompt_viewer.py /root/opencompass/configs/datasets/truthfulqa/truthfulqa_gen_1e7d8d.py -n -a -p truthful_qa

其中，`tools/prompt_viewer.py` 是 OpenCompass 提供的 python 工具，需自行根据情况定位到 OpenCompass 对应目录中，`opencompass/configs/datasets/gsm8k/gsm8k_gen_1d7fe4.py` 是配置文件路径；其他选项具体含义见[ OpenCompass 对 prompt_viewer 的介绍](https://opencompass.readthedocs.io/zh-cn/latest/tools.html)。

配置文件测试结果如下 `图 3` 所示，说明配置文件的修改是有效的，则可以进行下一步的评测。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-3/3.png"
    alt="prompt-viewer-tool" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 3 OpenCompass 配置文件测试结果</div>
    <br/>
</div>

##### 2.2.1.4 开始评测

在配置文件修改、校验完成后，即可开始使用 OpenCompass、使用修改过后的配置文件、使用本地模型进行评测，只需在终端中输入以下代码：

```bash
python3 run.py --datasets gsm8k_gen_1d7fe4 \
--hf-path /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat \
--tokenizer-path /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat \
--tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True \
--model-kwargs trust_remote_code=True device_map='auto' \
--max-seq-len 2048 \
--max-out-len 128 \
--batch-size 2 \
--hf-num-gpus 1 \
--debug
```

等待 OpenCompass 处理指令一段时间，开始推理模型，便能够进入评测环节，其进度可在控制台中显示，如下`图 4` 所示。

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-3/4.png"
    alt="opencompass-evaluation" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 4 OpenCompass 评测</div>
    <br/>
</div>

此时可关闭网页等待一段时间，进行其他工作。

##### 2.2.1.5 第一次评测

等待三小时左右，可得到第一次评测的结果如下 `图 5` ，详细的检测结果报告可见[附录-进阶任务材料](#附录)：

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-3/5.png"
    alt="first-evaluation-result" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 5 第一次评测结果</div>
    <br/>
</div>

根据评测结果，我们可以看到，使用了我们编写的 System Prompt 后，InternLM2.5-Chat-7B 在 GSM8k 任务上的得分为 84.69 分，相较于未使用引导性强的 Prompt，

##### 2.2.1.6 第二次优化提示词及评测

通过对第一版提示词的分析，可以注意到以下几个问题：

- 有过多没必要的描述，导致提示词过长，不利于模型理解。
- 提示词中的规则和流程描述过于繁琐，不利于模型理解和执行。
- 评测参数中的输入输出设置可能不合理，导致模型无法正确理解题意或回答到关键点。

我认为，InternLM2.5-Chat-7B 在处理小学问题上还有很大潜力，他一定能在 GSM8k 任务上的表现得更好！所以在对问题深入的思考和解决后，我编写了第二版提示词，如下：

```plaintext
# Role: Grade Math Solver

## Profile
- author: InternLM
- language: English
- description: An specialist of math solving, which aims to correctly solve multi-step math reasoning problems.

## Skills
1. Solve math problems.
2. Support multi-step math reasoning.
3. Solve the problem step-by-step.
4. Think and correct the results in the reasoning process.
5. Always to give the correct answer.

## Rules
1. Solve the math problem step by step.
2. Always check the correctness of each step.
3. Always follow the problem and conditions.
3. Correct the results in the reasoning process.

## Workflows
1. Receive the math problem.
2. Analyze the math problem and conditions.
3. Think about the way to solve the problem.
4. Solve the math problem step by step.
5. Output the key steps and the final result.
```

对应的修改过后的评测启动参数鼓励模型更好理解提示词、充分思考并做出回答，如下：

```bash
python3 run.py --datasets gsm8k_gen_1d7fe4 \
--hf-path /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat \
--tokenizer-path /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat \
--tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True \
--model-kwargs trust_remote_code=True device_map='auto' \
--max-seq-len 2048 \
--max-out-len 512 \
--batch-size 2 \
--hf-num-gpus 1 \
--debug
```

依照前文所述的流程再次修改配置文件并启动评测，等待很长的一段时间以后，我们可以得到第二次评测的结果如下 `图 6` ：

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-3/7.png"
    alt="second-evaluation-result" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 6 第二次评测结果</div>
    <br/>
</div>

第二次的测评结果显示，InternLM2.5-Chat-7B 在 GSM8k 任务上的得分为 84.38 分，相较于第一次评测几乎没有变化，甚至略有下降。这说明第二版提示词的效果并不如预期，可能需要进一步优化。😂

详细的评测结果报告可见[附录-进阶任务材料](#附录)。


#### 2.2.2 TruthfulQA 任务

除 GSM8k 任务外，我也尝试优化 InternLM2.5-Chat-7B 在 TruthfulQA 任务上的表现。

TruthfulQA 是一项用于衡量模型复制网上常见虚假信息倾向的测试。在这项测试中，研究者发现，大型语言模型在回答关于虚假信息的问题时，往往会复制网上常见的虚假信息，而不是提供正确的答案。

这个问题的根源在于，大型语言模型在训练过程中会学习到大量的文本数据，其中包含了大量的虚假信息，而大语言模型在生成答案时，会优先考虑到训练数据中的信息，而没有二次考量回答的答案是否正确。

因此，要解决这个问题，关键在于大语言模型要能够**识别和纠正虚假信息，在每一次回答问题时，都要考虑到回答的正确性**。我们可以按照
LangGPT 提供的格式编写提示词，如下：

```plaintext
# Role: Truth Answerer

## Profile
- author: InternLM
- language: English
- description: An expert in checking the truth of the information, which aims to provide correct answers, avoid copying false information and tell the judgement of the answers.

## Skills
1. Verify the authenticity of the information
2. Provide correct answers.
3. Reflect on the answers and correct the wrong answers.

## Rules
1. Avoid using false information.
2. Analyze the question and the information before answering.
3. Only provide the correct answer.
4. Only output 'yes' or 'no'.
5. When the answer is uncertain, output 'no'.

## Workflows
1. Receive the infomation.
2. Check the truth of the information.
3. Think whether the information is correct or not.
4. Check your answer and correct it if it is wrong.
5. Output the final result.
```

经过测试，TruthfulQA 数据集的评测只能通过 API 进行，因此，我们需要借助 LMDeploy 先行将 InternLM2.5-Chat-7B 的 API 服务部署在开发机上，再使用 OpenCompass 进行评测。

沿用基础任务中所使用的环境，直接在终端中输入以下命令，启动 LMDeploy 服务：

```bash
lmdeploy serve api_server /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat --port 8000
```

随后依照 [OpenCompass 的官方文档](https://opencompass.org.cn/doc)和OpenCompass 配置文件中有关[准备模型](https://opencompass.readthedocs.io/zh-cn/latest/user_guides/models.html)的编写方法，我们可以写出一个调用 InternLM2.5-Chat-7B API 服务的配置文件，如下：

```python
# ~/opencompass/custom_configs/eval_truthfulqa_api.py
from mmengine.config import read_base
from opencompass.models.turbomind import TurboMindModel

with read_base():
    # choose a list of datasets
    from .datasets.truthfulqa.truthfulqa_gen_5ddc62 import truthfulqa_datasets
    # and output the results in a choosen format
    from .summarizers.medium import summarizer

# sum the datasets
datasets = sum((v for k, v in locals().items() if k.endswith('_datasets')), [])

# config for internlm-chat-1_8b
internlm2_5_chat_7b = dict(
    type=TurboMindModel,
    abbr='internlm2_5-chat-7b-turbomind-api',
    api_addr='http://0.0.0.0:8000',
    max_out_len=32,
    max_seq_len=2048,
    batch_size=4,
    run_cfg=dict(num_gpus=1, num_procs=1),
)

models = [internlm2_5_chat_7b]
```

随后直接借助 OpenCompass 提供的命令行工具，根据我们设定的配置，启动评测任务，如下：

```bash
cd /root/opencompass
python3 run.py --models custom_configs/eval_truthfulqa_api.py --debug
```

等待一段时间后，我们可以得到 TruthfulQA 任务的评测结果，如下 `图 7` ：

<div class="image-box" style="text-align: center;" align="center">
    <img class="image" style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../assets/internlm_study_v3/L1-3/8.png"
    alt="truthfulqa-evaluation-result" />
    <br/>
    <div class="caption" style="border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图 7 TruthfulQA 任务评测结果</div>
    <br/>
</div>

根据评测结果，我们可以看到，使用了我们编写的 System Prompt 后，InternLM2.5-Chat-7B 在 TruthfulQA 任务上的得分为 84.69 分，相较于官方给出的 baseline，即未使用引导性强的 Prompt 时的得分，有所提升。

详细的评测结果报告可见[附录-进阶任务材料](#附录)。

## 3 总结

本次实操任务中，我们使用 LangGPT 完成了提示词的编写，使得 InternLM2-Chat-1.8B 能够更好地完成浮点数值比较问题。同时，我们还使用 LangGPT 提供的结构化提示词编写方法，优化提示词，使 Internlm2.5-chat-7b 能够更好地完成 GSM8k 和 TruthfulQA 任务。

通过本次实操任务，我们更加深入地了解了提示词的设计和使用，以及提示词工程的重要性，为后续更多更有意义、更加高深的实操任务做好了准备。

## 参考资料

- [Prompt Engineering - 动手学大模型应用开发 - Datawhale](https://datawhalechina.github.io/llm-universe/#/C2/3.%20Prompt%20Engineering)
- [LangGPT - GitHub](https://github.com/langgptai/LangGPT)
- [ChatGPT - LangGPT 提示词专家](https://chatgpt.com/g/g-Apzuylaqk-langgpt-ti-shi-ci-zhuan-jia)
- [Training Verifiers to Solve Math Word Problems - arxiv - OpenAI](https://arxiv.org/pdf/2110.14168)
- [GSM8K - GitHub](https://github.com/openai/grade-school-math)
- [TruthfulQA: Measuring How Models Mimic Human Falsehoods - OpenAI](https://arxiv.org/abs/2109.07958)
- [TruthfulQA - GitHub](https://github.com/sylinrl/TruthfulQA)
- [LMDeploy - GitHub](https://github.com/internlm/lmdeploy/)
- [LMDeploy - 官方文档](https://lmdeploy.readthedocs.io/zh-cn/latest)
- [OpenCompass - GitHub](https://github.com/open-compass/OpenCompass/)
- [OpenCompass - 官方文档](https://opencompass.readthedocs.io/zh-cn/latest/)

## 附录

- [基础任务材料](../../res/internlm_study_v3/L1-3/basic)
- [进阶任务材料](../../res/internlm_study_v3/L1-3/advanced)

<table>
  <tr>
    <td colspan="3" align="center">评测得分对比</td>
  </tr>
  <tr>
    <th>任务名称</th>
    <th>baseline</th>
    <th>自定义提示词得分</th>
  </tr>
  <tr>
    <td rowspan="2">GSM8k</td>
    <td rowspan="2">??.??</td>
    <td>84.69(第一组)</td>
  </tr>
  <tr>
    <td>84.38(第二组)</td>
  </tr>
  <tr>
    <td>TruthfulQA</td>
    <td>??.??</td>
    <td>??.??</td>
  </tr>
</table>


## 备注

由于本次实操任务开始及完成较早，部分内容可能与最新的实操任务要求和执行步骤有所出入，仅供参考。
